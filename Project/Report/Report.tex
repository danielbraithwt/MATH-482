\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\title{%
	MATH 482\\
	\large Matrix Factorisation Project \\
	\small Code Available: https://github.com/danielbraithwt/MATH-482
}
\author{Daniel Braithwaite}


\begin{document}

\maketitle

\section{Introduction}
To dicuss the idea of matrix factorisation and methods to solve it first we must understand the motivation for wanting to solve such a problem. In the case of the Netflicks challenge the problem was to build a system to recomend movies to users. We have this very large martix $R$ with the rows corosponding to a user and a column corosponding to a movie. The entry $R_{i,j}$ is the rating that user i gave movie j, in practice we would find that a very small percentage of this matrix would be filled in. To make recomendations we would like to predict the ratings which a user might give a movie which they havent watched.\\

\section{Matrix Factorization Solutions}
\subsection{Solution 1: $R = U \cdot M$}
The first soluton we consider is that R (an $u x m$ matrix) is actually the product of two smaller matricies $U$ and $M$. Where $U$ (a $u x k$ matrix) represents the users in some latent feature space and $M$ (a $m x k$ matrix) represents the movies in the latent feature space. We consider $M_{i,j}$ to be the ammount movie $i$ has feature $j$, likewise we consider $U_{i,j}$ to be how much user i is interested in movies with feature j. Then we can take the rating user i gives movie j to be $\hat{R}_{i,j} = row(U, i)^T \cdot row(M, j)$. Now the problem becomes how do we learn these matricies $U$ and $M$.\\

We consider the folowing optimizimation problem, where $G$ contains all pairs $(i,j)$ for which we know $R_{i,j}$

\begin{equation*}
\begin{aligned}
& \underset{U, M}{\text{arg min}}
& & \sum_{(i,j) \in G} (R_{i,j} - row(U, i)^T \cdot row(M, j))^2  \\
\end{aligned}
\end{equation*}

This optimization problem can be solved with gradient decent. We generate a matrix $R$ which is (20x23) by multiplying two randomly generated matrcies $U$ (20 x 15) and $M$ (23 by 15). Initially the training set and test set are the same and we train over the entire data set. We train each situation 10 times each from random initial conditions.

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c |}
\hline
latent factors & peformance (SSE) & 95\% CI \\
\hline
\hline
5 & 10.675 & (10.648, 10.702)\\
10 & 0.767 & (0.767, 0.767)\\
15 & 0.053 & (0.039, 0.066) \\
\hline
\end{tabular}
\caption{Table for latent features vs peformance, over entire data set}
\end{table}

However in a real world situation we know that the problems which utilize matrix factorization usially involve factorizing sparse matricies, so what happens when we remove say half the entries. Now our data is split in two, half is our training data and the other half is our test data. The results below are generated by training with the same hyperparamaters as before

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c | c |}
\hline
latent factors & training (SSE) &  training 95\% CI & test (SSE) &  test 95\% CI  \\
\hline
\hline
5 & 0 & 0 & 0 & 0\\
10 & 0 & 0 & 0 & 0\\
15 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}
\caption{Table for latent features vs peformance, over partial data set}
\end{table}

We observe that the test error is higher than the training error, and these differences are stastically significant which indicates that there is overfitting occouring.

One common approach to solve the overfitting that occours in this type of matrix factorization is to use regularization on the matricies $U$ and $M$. Making our optimization problem the folowing

\begin{equation*}
\begin{aligned}
& \underset{U, M}{\text{arg min}}
& & \big[ \sum_{(i,j) \in G} (R_{i,j} - row(U, i)^T \cdot row(M, j))^2 \big] + \lambda (\lVert U \lVert_2 + \lVert M \lVert_2)\\
\end{aligned}
\end{equation*}

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c | c |}
\hline
latent factors & training (SSE) &  training 95\% CI & test (SSE) &  test 95\% CI  \\
\hline
\hline
5 & 19.783 & (19.692, 19.875)& 37.965 & (37.830, 38.100)\\
10 & 19.423 & (19.375, 19.473) & 38.133 & (38.002, 38.264)\\
15 & 19.347 & (19.323, 19.372) & 38.171 & (38.117, 38.225)\\
\hline
\end{tabular}
\caption{Table for latent features vs peformance, over partial data set with regulrization}
\end{table}

While this does provide a reduction in overfitting it is sill a significant problem. The next concept we can implement is that of biases, our model that we are learning is suppose to capture the interactions between the users and movies however we might find that in some cases a perticular user gives mostly low ratings or a perticular movie generally recieves low ratings, these propertys arnt interactions between the users and movies, prehapse the user is just harsh or the movie is simply bad. Biases capture this idea so the model can learn what is truely important.

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c | c |}
\hline
latent factors & training (SSE) &  training 95\% CI & test (SSE) &  test 95\% CI  \\
\hline
\hline
5 & 0 & 0 & 0 & 0\\
10 & 0 & 0 & 0 & 0\\
15 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}
\caption{Table for latent features vs peformance, over partial data set with regulirzation and biases}
\end{table}

\subsection{Solution 2: Using Neural Networks}

In this section we present two similar solutions each using neural networks, only difference being whether we use two neural networks or one.

\subsubsection{Two Neural Networks}
In the same set up as before there is a matrix R with rows representing users and columns representing movies. Our aim is to optimize the folowing. Take two nerual networks $f_{\theta}$ which takes a row of R to some latent feature space and $f_{\phi}$ which takes columns of R to some feature space. Then we compute the ranking user $i$ gives movie $j$ by the folowing $\hat{R}_{i,j} = f_{\theta}(user_i)^T \cdot f_{\phi}(movie_j)$. Giving us the folowing optimization problem (where $G$ is defined as before)

\begin{equation*}
\begin{aligned}
& \underset{\theta, \phi}{\text{arg min}}
& & \sum_{(i,j) \in G} (R_{i,j} - f_{\theta}(user_i)^T \cdot f_{\phi}(movie_j))^2  \\
\end{aligned}
\end{equation*}

\subsubsection{Single Neural Network}
This approach is very similar to the one just presented, how ever insted now we only have one neural network $f_{\psi}$, which takes some row of R representing a user and some column of R representing a movie and outputs a rating. Making our approximation of ratings $\hat{R}_{i,j} = f_{\psi}(user_i, movie_j)$, and finally giving us the folowing optimization problem.

\begin{equation*}
\begin{aligned}
& \underset{\theta, \phi}{\text{arg min}}
& & \sum_{(i,j) \in G} (R_{i,j} - f_{\psi}(user_i, movie_j))^2  \\
\end{aligned}
\end{equation*}


\section{Factorization Method Comparason}
We wish to compare the peformance of these methods against each other and identify the tradeoffs between them. Testing our factorization methods on randomly generated data is a good way to develop an understadning in a small environment but really we want to see how these methods peform on real data. We will be using the MovieLens 100K data set, consiting of 943 users and 1682 movies where each user has rated atleast 20 movies.


\subsection{Conclusions}



\end{document}